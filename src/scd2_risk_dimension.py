"""
üìä SCD Type 2 (Slowly Changing Dimension) ‚Äî Dimens√£o de Risco Financeiro
======================================================================

‚úÖ O que este job faz (vis√£o leiga e objetiva):
- L√™ uma tabela "snapshot" com o estado mais recente das contas (fonte operacional).
- Para cada grupo de conta (account_group_id), pega o registro mais recente (pelo ingested_at).
- Compara esse estado com o registro "ativo" atual na dimens√£o SCD2 (is_current = true).
- Se mudou o risco (ou qualquer atributo que voc√™ decidir versionar), ele:
  1) Encerra a vers√£o anterior (valid_to = data_atual - 1; is_current = false)
  2) Insere uma nova vers√£o (valid_from = data_atual; valid_to = 9999-12-31; is_current = true)
- Se n√£o mudou, n√£o faz nada.
- √â idempotente: se voc√™ rodar de novo no mesmo dia, n√£o duplica vers√µes.

üìå Por que isso √© SCD2 de verdade?
- Porque ele N√ÉO sobrescreve o passado.
- Ele mant√©m cada vers√£o com vig√™ncia (valid_from/valid_to) + flag de corrente (is_current).

‚ö†Ô∏è Observa√ß√£o:
- Aqui usamos "data" (current_date) como refer√™ncia de vig√™ncia.
  Se voc√™ quiser granularidade por timestamp (mais preciso), d√° para ajustar para current_timestamp.

Tabelas (fict√≠cias, portf√≥lio):
- Fonte:   financas.ops_finance.ar_open_items
- Destino: financas.ops_finance.dim_ar_risk_scd2
"""

from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.window import Window
from delta.tables import DeltaTable

# ============================================================
# 0) Spark Session + configura√ß√µes
# ============================================================
spark = SparkSession.builder.appName("scd2_financial_risk_dimension").getOrCreate()

# Desliga ANSI para evitar que algumas convers√µes invalidem o job (trade-off: menos rigor).
spark.conf.set("spark.sql.ansi.enabled", "false")

# ============================================================
# 1) Contexto do Databricks (catalog/schema) e tabelas
# ============================================================
# Par√¢metros de execu√ß√£o (quando rodar como Job/Workflow no Databricks)
run_params = dbutils.notebook.entry_point.getCurrentBindings()

# Cat√°logo padr√£o do portf√≥lio (se n√£o vier via job param)
catalog = run_params.get("catalog") or "financas"
schema = "ops_finance"

# Tabela fonte (snapshot operacional) e tabela destino (dimens√£o SCD2)
source_table = f"{catalog}.{schema}.ar_open_items"
target_table = f"{catalog}.{schema}.dim_ar_risk_scd2"

# Ajusta o contexto do Spark no Unity Catalog
spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE {schema}")

print(f"[INFO] source_table: {source_table}")
print(f"[INFO] target_table: {target_table}")

# ============================================================
# 2) Leitura da fonte e sele√ß√£o do registro mais recente por chave
# ============================================================
df_src = spark.table(source_table)

# Data de vig√™ncia do run (SCD2 por dia)
run_date = F.current_date()

# Para cada account_group_id, pegamos o registro mais recente pelo ingested_at
w_latest = Window.partitionBy("account_group_id").orderBy(F.col("ingested_at").desc())

df_latest = (
    df_src
    .withColumn("rn", F.row_number().over(w_latest))
    .filter(F.col("rn") == 1)
    .drop("rn")
)

# ============================================================
# 3) Change Detection: hash determin√≠stico dos atributos versionados
# ============================================================
# Aqui estamos versionando apenas o delinquency_risk_level.
# Se quiser versionar mais colunas, inclua no concat_ws("||", ...)
df_latest = df_latest.withColumn(
    "scd_hash",
    F.sha2(
        F.concat_ws(
            "||",
            F.coalesce(F.col("delinquency_risk_level").cast("string"), F.lit(""))
        ),
        256
    )
)

# Campos padr√£o SCD2
infinite_date = F.lit("9999-12-31").cast("date")

df_incoming = (
    df_latest
    .withColumn("valid_from", run_date)
    .withColumn("valid_to", infinite_date)
    .withColumn("is_current", F.lit(True))
    .withColumn("created_at", F.current_timestamp())
    .withColumn("updated_at", F.current_timestamp())
)

# ============================================================
# 4) Checar se a tabela destino existe
# ============================================================
tables = spark.catalog.listTables(schema)
target_exists = any(t.name == "dim_ar_risk_scd2" for t in tables)

# ============================================================
# 5) Se n√£o existir, fazemos carga inicial (primeira vers√£o)
# ============================================================
if not target_exists:
    (
        df_incoming
        .write.format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .saveAsTable(target_table)
    )
    print("[OK] Tabela SCD2 criada e carga inicial conclu√≠da.")
else:
    # ============================================================
    # 6) SCD2 Incremental: fechar vers√£o atual + inserir nova vers√£o
    # ============================================================
    delta = DeltaTable.forName(spark, target_table)

    # Recupera somente registros atuais (ativos) da dimens√£o
    df_current = (
        spark.table(target_table)
        .filter(F.col("is_current") == True)
        .select("account_group_id", "scd_hash", "valid_from")
        .withColumnRenamed("scd_hash", "curr_hash")
        .withColumnRenamed("valid_from", "curr_valid_from")
    )

    # Compara o hash do "incoming" com o hash do registro atual
    # Classifica:
    # - NEW: chave ainda n√£o existe na dimens√£o
    # - CHANGED: chave existe, mas hash √© diferente (houve mudan√ßa)
    # - NO_CHANGE: nada mudou
    df_changes = (
        df_incoming.alias("s")
        .join(df_current.alias("t"), on="account_group_id", how="left")
        .withColumn(
            "change_type",
            F.when(F.col("t.curr_hash").isNull(), F.lit("NEW"))
             .when(F.col("s.scd_hash") != F.col("t.curr_hash"), F.lit("CHANGED"))
             .otherwise(F.lit("NO_CHANGE"))
        )
    )

    # Mant√©m s√≥ o que realmente precisa alterar/inserir
    df_to_upsert = df_changes.filter(F.col("change_type").isin("NEW", "CHANGED"))

    # ============================================================
    # 6A) Fechar registros atuais (somente para CHANGED)
    # ============================================================
    # Encerramos a vig√™ncia no dia anterior ao run_date
    df_to_close = (
        df_to_upsert
        .filter(F.col("change_type") == "CHANGED")
        .select("account_group_id")
        .distinct()
        .withColumn("close_to", F.date_sub(run_date, 1))
        .withColumn("closed_at", F.current_timestamp())
    )

    # Merge para fechar a vers√£o atual (is_current=true)
    # S√≥ fecha se encontrar o registro ativo daquela chave
    delta.alias("t").merge(
        df_to_close.alias("s"),
        "t.account_group_id = s.account_group_id AND t.is_current = true"
    ).whenMatchedUpdate(set={
        "valid_to": "s.close_to",
        "is_current": "false",
        "updated_at": "s.closed_at"
    }).execute()

    # ============================================================
    # 6B) Inserir nova vers√£o para NEW e CHANGED (idempotente)
    # ============================================================
    # Evita duplicar se rodar de novo no mesmo dia:
    # - verifica se j√° existe vers√£o atual com valid_from = run_date
    df_loaded_today = (
        spark.table(target_table)
        .filter((F.col("is_current") == True) & (F.col("valid_from") == run_date))
        .select("account_group_id", "scd_hash")
        .withColumnRenamed("scd_hash", "loaded_hash")
    )

    # Seleciona somente vers√µes que ainda n√£o foram inseridas hoje
    final_cols = df_incoming.columns  # garante a proje√ß√£o final correta

    df_to_insert = (
        df_to_upsert
        .select([F.col(c) for c in final_cols] + ["change_type"])
        .join(df_loaded_today, on="account_group_id", how="left")
        .filter(F.col("loaded_hash").isNull() | (F.col("loaded_hash") != F.col("scd_hash")))
        .drop("loaded_hash", "change_type")
    )

    # Insere novas vers√µes (append)
    (
        df_to_insert
        .write.format("delta")
        .mode("append")
        .saveAsTable(target_table)
    )

    print("[OK] SCD2 executado com sucesso (fechamento + inser√ß√£o de novas vers√µes).")

# ============================================================
# 7) Observa√ß√£o sobre reten√ß√£o
# ============================================================
# Em SCD2, geralmente N√ÉO se apaga hist√≥rico.
# Se voc√™ quiser reten√ß√£o (ex.: 24 meses) por custo, implemente com cuidado,
# para n√£o quebrar auditoria e an√°lises temporais.

print("[DONE] Pipeline finalizado.")
